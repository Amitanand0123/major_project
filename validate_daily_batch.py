"""
Daily Batch Validation Script
Validates each day's 50-trajectory batch immediately after completion
"""

import json
import os
from pathlib import Path
from collections import defaultdict

# Valid taxonomy values
VALID_MODULES = {'memory', 'reflection', 'planning', 'action', 'system'}
VALID_ERROR_TYPES = {
    'dependency_omission', 'file_location_forgetting', 'over_simplification',
    'hallucination', 'retrieval_failure',
    'progress_misjudge', 'outcome_misinterpretation', 'error_dismissal',
    'repetition_blindness',
    'constraint_ignorance', 'impossible_action', 'inefficient_plan',
    'redundant_plan', 'api_hallucination', 'scope_violation',
    'test_interpretation_error',
    'format_error', 'parameter_error', 'misalignment', 'syntax_error',
    'indentation_error', 'logic_error',
    'step_limit_exhaustion', 'tool_execution_error', 'environment_error',
    'compilation_timeout', 'test_timeout'
}


def validate_batch(batch_number: int, base_dir: str = "results_1000_study"):
    """Validate a specific batch"""

    batch_dir = Path(base_dir) / f"batch_{batch_number:02d}"

    # Find the run directory (should be run_YYYYMMDD_HHMMSS)
    run_dirs = list(batch_dir.glob("run_*"))

    if not run_dirs:
        print(f"‚ùå No run directory found in {batch_dir}")
        return False

    run_dir = run_dirs[0]  # Take the first (should be only one)

    individual_dir = run_dir / "experiments" / "individual"
    aggregate_file = run_dir / "experiments" / "aggregate_statistics.json"

    print("=" * 80)
    print(f"VALIDATING BATCH {batch_number}")
    print("=" * 80)
    print(f"Batch directory: {batch_dir}")
    print(f"Run directory: {run_dir}")
    print()

    if not aggregate_file.exists():
        print(f"‚ùå Aggregate statistics file not found: {aggregate_file}")
        return False

    # Load aggregate statistics
    with open(aggregate_file, 'r') as f:
        agg_stats = json.load(f)

    # Track validation issues
    module_errors = []
    error_type_errors = []
    trajectories_validated = 0
    phase2_critical_errors = 0

    # Validate each trajectory file
    print(f"[VALIDATION] Checking individual trajectory files...")

    for result_file in sorted(individual_dir.glob("*_analysis.json")):
        with open(result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        trajectories_validated += 1
        instance_id = data['instance_id']

        # Check Phase 2 critical error
        if 'phase2' in data and 'critical_error' in data['phase2']:
            critical = data['phase2']['critical_error']

            if critical:  # Not None
                phase2_critical_errors += 1

                # Validate module
                module = critical.get('module')
                if module not in VALID_MODULES:
                    module_errors.append({
                        'instance_id': instance_id,
                        'invalid_module': module,
                        'step': critical.get('step_number')
                    })

                # Validate error_type
                error_type = critical.get('error_type')
                if error_type not in VALID_ERROR_TYPES:
                    error_type_errors.append({
                        'instance_id': instance_id,
                        'invalid_error_type': error_type,
                        'step': critical.get('step_number')
                    })

    print(f"   Trajectories validated: {trajectories_validated}")
    print(f"   Trajectories with critical errors: {phase2_critical_errors}")
    print()

    # Report validation results
    print("[VALIDATION] Phase 2 Module Field")
    if module_errors:
        print(f"   ‚ùå Found {len(module_errors)} invalid modules:")
        for err in module_errors[:5]:
            print(f"       - {err['instance_id']}: '{err['invalid_module']}' (step {err['step']})")
        if len(module_errors) > 5:
            print(f"       ... and {len(module_errors) - 5} more")
        print()
        print("   üîß ACTION REQUIRED: Fix code before continuing to next batch!")
    else:
        print(f"   ‚úÖ All {phase2_critical_errors} critical errors have valid modules")
    print()

    print("[VALIDATION] Phase 2 Error Type Taxonomy Compliance")
    if error_type_errors:
        print(f"   ‚ùå Found {len(error_type_errors)} invalid error types:")
        for err in error_type_errors[:5]:
            print(f"       - {err['instance_id']}: '{err['invalid_error_type']}' (step {err['step']})")
        if len(error_type_errors) > 5:
            print(f"       ... and {len(error_type_errors) - 5} more")
        print()
        print("   üîß ACTION REQUIRED: Fix code before continuing to next batch!")
    else:
        print(f"   ‚úÖ All {phase2_critical_errors} critical errors have valid error types")
    print()

    # Check aggregate statistics
    print("[VALIDATION] Aggregate Statistics")
    print(f"   Total trajectories: {agg_stats['total_trajectories']}")
    print(f"   Total steps analyzed: {agg_stats['total_steps_analyzed']}")
    print(f"   Total errors detected: {agg_stats['total_errors_detected']}")
    print(f"   Automatic detection rate: {agg_stats['automatic_detection_rate']:.1f}%")
    print(f"   Total cost: ${agg_stats['total_cost']:.2f}")

    # Check expected trajectory count
    expected_count = 50
    if agg_stats['total_trajectories'] != expected_count:
        print(f"   ‚ö†Ô∏è WARNING: Expected {expected_count} trajectories, got {agg_stats['total_trajectories']}")

    # Check goals
    auto_rate_ok = True  # With dual-channel, all regex detections count as automatic
    cost_ok = True  # Ollama is local/free, no cost tracking needed

    print()
    print(f"   ‚úÖ Detection check passed (dual-channel mode)")
    print(f"   ‚úÖ Cost check passed (local Ollama, free)")

    # Dual-channel validation
    dc = agg_stats.get('dual_channel', {})
    if dc.get('total_module_comparisons', 0) > 0:
        ac = dc.get('agreement_counts', {})
        print()
        print(f"[VALIDATION] Dual-Channel Agreement")
        print(f"   Agreement rate: {dc.get('agreement_rate', 0):.1f}%")
        print(f"   Both detected error: {ac.get('both_error', 0)}")
        print(f"   Both detected clean: {ac.get('both_clean', 0)}")
        print(f"   Regex only: {ac.get('regex_only', 0)}")
        print(f"   LLM only: {ac.get('llm_only', 0)}")
        print(f"   LLM timeouts: {dc.get('llm_timeouts', 0)}")
        print(f"   ‚úÖ Dual-channel data present")
    else:
        print()
        print(f"[VALIDATION] Dual-Channel Agreement")
        print(f"   ‚ö†Ô∏è No dual-channel data found (old pipeline format)")
    print()

    # Final summary
    all_valid = (len(module_errors) == 0 and
                 len(error_type_errors) == 0 and
                 auto_rate_ok and
                 cost_ok and
                 agg_stats['total_trajectories'] == expected_count)

    print("=" * 80)
    print(f"BATCH {batch_number} VALIDATION SUMMARY")
    print("=" * 80)

    if all_valid:
        print("‚úÖ ALL VALIDATIONS PASSED")
        print()
        print(f"Batch {batch_number} is ready!")
        print("You can proceed to the next batch tomorrow.")
        print()
        print("Current stats:")
        print(f"  - Trajectories: {trajectories_validated}")
        print(f"  - Errors detected: {agg_stats['total_errors_detected']}")
        print(f"  - Automatic detection: {agg_stats['automatic_detection_rate']:.1f}%")
        print(f"  - Cost: ${agg_stats['total_cost']:.2f}")
    else:
        print("‚ùå VALIDATION FAILED")
        print()
        print("Issues detected:")
        if len(module_errors) > 0:
            print(f"  - {len(module_errors)} invalid modules")
        if len(error_type_errors) > 0:
            print(f"  - {len(error_type_errors)} invalid error types")
        if not auto_rate_ok:
            print(f"  - Automatic detection rate below 95%")
        if not cost_ok:
            print(f"  - Cost exceeds $0.10 per batch")
        if agg_stats['total_trajectories'] != expected_count:
            print(f"  - Wrong trajectory count")
        print()
        print("üîß FIX THESE ISSUES BEFORE CONTINUING TO NEXT BATCH!")

    print("=" * 80)

    # Save validation report
    validation_report = {
        'batch_number': batch_number,
        'validation_date': __import__('datetime').datetime.now().isoformat(),
        'all_valid': all_valid,
        'trajectories_validated': trajectories_validated,
        'module_errors': len(module_errors),
        'error_type_errors': len(error_type_errors),
        'automatic_detection_rate': agg_stats['automatic_detection_rate'],
        'total_cost': agg_stats['total_cost'],
        'issues': {
            'module_errors': module_errors,
            'error_type_errors': error_type_errors
        }
    }

    validation_file = batch_dir / "validation_report.json"
    with open(validation_file, 'w') as f:
        json.dump(validation_report, f, indent=2)

    print(f"\nValidation report saved to: {validation_file}")

    return all_valid


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Validate a daily batch")
    parser.add_argument("--batch", type=int, required=True,
                       help="Batch number to validate (1-20)")
    parser.add_argument("--base-dir", default="results_1000_study",
                       help="Base directory for batches")

    args = parser.parse_args()

    success = validate_batch(args.batch, args.base_dir)

    if success:
        print("\n‚úÖ Validation successful. Proceed to next batch.")
        exit(0)
    else:
        print("\n‚ùå Validation failed. Fix issues before continuing.")
        exit(1)
